{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: import necessary libraries\n",
    "from selenium import webdriver  \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Browerser Driver\n",
    "browser = webdriver.Chrome('windows/chromedriver')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium to navigate the site and Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text \n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "browser.get(url)\n",
    "html = browser.page_source\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.body.prettify())\n",
    "\n",
    "# Access the latest news with CSS selectors\n",
    "news_list = soup.find_all('div', class_ = 'list_text')\n",
    "latestnews_title = news_list[0].find('div', class_ = 'content_title').a.text\n",
    "latestnews_p = news_list[0].find('div', class_ = 'article_teaser_body').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium to navigate the site and Scrape the image url for the current Featured Mars Image\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.get(url)\n",
    "html = browser.page_source\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.body.prettify())\n",
    "\n",
    "# Access the image url with CSS selectors\n",
    "imageurl_list = soup.find_all('footer')\n",
    "featured_image_url = 'https://www.jpl.nasa.gov' + imageurl_list[0].find('a', class_='button fancybox')['data-fancybox-href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium to navigate the site and Scrape the latest Mars weather tweet from the page\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.get(url)\n",
    "html = browser.page_source\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.body.prettify())\n",
    "\n",
    "# Access the latest Mars weather tweet with CSS selectors\n",
    "tweet_list = soup.find_all('div',class_ = 'js-tweet-text-container')\n",
    "latestmarsweather_tweet = tweet_list[0].find('p').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://space-facts.com/mars/'\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Save data from the table containing facts into Panda DataFrame\n",
    "df = tables[0]  \n",
    "df.columns = ['fact', 'value']\n",
    "df.set_index('fact', inplace = True)\n",
    "\n",
    "# Use Pandas to convert the data to a HTML table string\n",
    "html_table = df.to_html()\n",
    "html_table.replace('\\n', '')\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html_table, 'html.parser').tbody\n",
    "\n",
    "tablerow_list = soup.find_all('tr')\n",
    "tablerowth_list = soup.find_all('th')\n",
    "tablerowtd_list = soup.find_all('td')\n",
    "\n",
    "tablerowthvl_list = []\n",
    "tablerowtdvl_list = []\n",
    "tablerowthvlvl_list = []\n",
    "tablerowtdvlvl_list = []\n",
    "\n",
    "for r in tablerowth_list:\n",
    "    n = str(r)[4:]\n",
    "    tablerowthvl_list.append(n)\n",
    "for r in tablerowthvl_list:\n",
    "    n = str(r)[:-6]\n",
    "    tablerowthvlvl_list.append(n)    \n",
    "\n",
    "for r in tablerowtd_list:\n",
    "    n = str(r)[4:]\n",
    "    tablerowtdvl_list.append(n)\n",
    "for r in tablerowtdvl_list:\n",
    "    n = str(r)[:-6]\n",
    "    tablerowtdvlvl_list.append(n)    \n",
    "\n",
    "th_and_td = zip(tablerowthvlvl_list,tablerowtdvlvl_list)\n",
    "\n",
    "thtd_dict = {}\n",
    "thtd_dictlist = []\n",
    "\n",
    "for i in th_and_td:\n",
    "    l = list(i)\n",
    "    thtd_dict[\"fact\"] = l[0]\n",
    "    thtd_dict[\"value\"] = l[1]\n",
    "    thtd_dictlist.append(thtd_dict.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium to navigate the site and Scrape high resolution images for each of Mar's hemispheres\n",
    "\n",
    "# URL of page to be scraped\n",
    "url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.get(url)\n",
    "html = browser.page_source\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(soup.body.prettify())\n",
    "\n",
    "hemisphere_dictlist = []\n",
    "hemisphere_dict = {}\n",
    "item_list = soup.find_all('div', class_ = \"item\")\n",
    "# print(soup.body.prettify() ) \n",
    "\n",
    "# Access the high resolution images for each of Mar's hemispheres with CSS selectors\n",
    "for m in item_list:\n",
    "    hemisphere_dict[\"title\"] = m.div.a.h3.text\n",
    "    pageurlstr = \"https://astrogeology.usgs.gov\" + m.div.a[\"href\"] \n",
    "    browser.get(pageurlstr)   \n",
    "    htmlnew = browser.page_source\n",
    "    soup = bs(htmlnew, 'html.parser').body\n",
    "    imageurlstr = \"https://astrogeology.usgs.gov\" + soup.find('img', class_ = \"wide-image\")[\"src\"]\n",
    "    hemisphere_dict[\"img_url\"] = imageurlstr\n",
    "    hemisphere_dictlist.append(hemisphere_dict.copy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
